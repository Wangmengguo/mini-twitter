# AI 基础设施健康监控报告
**生成时间**: 2026-02-10 23:00  
**监控周期**: 过去 60 分钟

---

## 📊 整体状态概览

| Provider | 状态 | 关键模型健康 | 备注 |
|----------|------|--------------|------|
| 🔧 Local Proxy | 🟡 Degraded | Opus 4.6: 2.5s (超阈值) | 延迟波动，已触发降级 |
| 🌐 Remote API | ✅ Healthy | Opus/Sonnet 正常 | 稳定 |
| 📡 OpenCode-Zen | 🔴 Partial | Kimi/GLM 不稳定 | 频繁抖动 |
| 🌍 OpenRouter | ✅ Healthy | Pony Alpha 正常 | 新增监控 |

---

## 🔴 故障分析：OpenCode-Zen 频繁抖动

### 问题模型
- **GLM-4.7-free** (22:00 - 22:57)
- **Kimi-k2.5-free** (22:57 - 23:00)

### 故障特征
1. **瞬时拒绝**：请求在 ~180ms 内返回错误（非超时）
2. **状态切换频繁**：
   - 22:00: GLM down → 22:57: GLM up
   - 22:57: Kimi up → 23:00: Kimi down
3. **同 Provider 其他模型正常**：Minimax 持续可用

### 根因推断
基于延迟模式和错误特征，判断为 **OpenCode-Zen 后端流控策略**：

1. **动态限流 (QPS Throttling)**  
   - 后端对每个模型实施独立的请求频率限制
   - 触发后立即返回 4xx (可能是 429 Too Many Requests 或 403 Forbidden)
   - 延迟 <200ms 说明在 API Gateway 层直接拒绝，未到达模型

2. **轮转式熔断**  
   - 为防止特定模型过载，后端可能采用"轮流休息"策略
   - GLM 恢复时，Kimi 进入冷却期
   - 符合免费渠道的资源调度模式

3. **账户级配额耗尽**  
   - OpenCode-Zen 免费层可能有每日/每小时的总调用限制
   - 达到阈值后，随机拒绝部分模型的请求

### 验证建议
1. 在下次 Kimi down 时，立即 curl 获取完整错误响应体
2. 监控是否存在固定时间窗口（如每小时 00 分重置）
3. 检查是否有"公平使用政策"文档

---

## 🟡 性能降级：Local Proxy

### 延迟突增
- **Opus 4.6**: 1.06s → 2.51s (+136%)
- **Sonnet 4.5**: 1.26s → 1.98s (+57%)

### 可能原因
1. **本地代理负载**：`localhost:7861` 可能正在处理其他密集请求
2. **上游 Gemini API 拥塞**：Local Proxy 转发到 Gemini，而 Gemini 响应变慢
3. **网络抖动**：家庭网络或 ISP 路由不稳定

### 对比
- **Remote API (gcli2api-ag)** 的 Opus 4.6 仅 534ms，说明问题在本地环节

---

## ✅ 新增监控：OpenRouter Pony Alpha

- **状态**: 正常运行
- **首次探测延迟**: 2.47s
- **模型定位**: 免费公共模型
- **后续观察**: 需积累 24 小时数据建立基准

---

## 📌 行动建议

### 立即
1. ✅ **已完成**: OpenRouter Pony Alpha 已纳入监控面板
2. 🔧 **调查 Local Proxy**: 检查 `localhost:7861` 进程资源占用

### 短期 (24h)
1. 📊 收集 OpenCode-Zen 完整错误日志，确认限流策略
2. ⚙️ 考虑调整探测频率：对不稳定模型降低到 30 分钟/次

### 长期
1. 🔄 增加容错配置：当 Kimi down 时自动切换到 GLM 或 Minimax
2. 📈 建立延迟基准线，自动识别异常波动

---

**报告生成**: 基于过去 60 分钟的 3 次探测数据  
**下次更新**: 自动（状态变化时触发推送）
